To connectspark spark-sql --master yarn --conf spark.ui.port=4041 
in our case, we can use hive directly

When we create a database -> create database kazaparv_retail_db_txt;
the database will be created. However lets see what happens in hive metastore

//hive> set hive.metastore.warehouse.dir;
hive.metastore.warehouse.dir=/user/hive/warehouse

Then do a 

// dfs -ls /user/hive/warehouse/kazaparv_retail_db_txt.db;

scenario:1 create a table YOUR_NAME_retail_db_txt and load tables orders, order_items in your database.

// As the database is created, let us check the columns that are imported into hive database
create table orders (
     order_id int,
     order_date string,
     order_customer_id int,
     order_status string
     ) row format delimited fields terminated by ','
	 stored as textfile;

- Now ,as the table is created, lets load the data in to this table

// load data local inpath '/home/kazaparv_gmail_com/data-master/data-master/retail_db/orders' into table orders;

- If you are copying the file from HDFS path, then we need to remove local and then give the appropriate path
- if you wanted to overiwte the table. We can use 'overwrite' keyword 'overwrite into'


- Once we run the load command, we should see part file created in metastore
	- /user/hive/warehouse/kazaparv_retail_db_txt.db/orders/part-00000
	
// Also, we can run the command "select * from orders limit 10", to verify the comma seperated data
		1       2013-07-25 00:00:00.0   11599   CLOSED
		2       2013-07-25 00:00:00.0   256     PENDING_PAYMENT
		3       2013-07-25 00:00:00.0   12111   COMPLETE  
		4       2013-07-25 00:00:00.0   8827    CLOSED    
		5       2013-07-25 00:00:00.0   11318   COMPLETE  
		6       2013-07-25 00:00:00.0   7130    COMPLETE  
		7       2013-07-25 00:00:00.0   4530    COMPLETE  
		8       2013-07-25 00:00:00.0   2911    PROCESSING
		9       2013-07-25 00:00:00.0   5657    PENDING_PAYMENT
		10      2013-07-25 00:00:00.0   5648    PENDING_PAYMENT 
		
- Now. lets create order_items

//create table order_items (
     order_item_id int,
     order_item_order_id int,
     order_item_product_id int,
     order_item_quantity int,
	 order_item_subtotal float,
	 order_item_product_price float
     ) row format delimited fields terminated by ','
	 stored as textfile
	 
- Now, load the order_items details


// load data local inpath '/home/kazaparv_gmail_com/data-master/data-master/retail_db/order_items' into table order_items;

// Also, we can run the command "select * from order_items limit 10", to verify the comma seperated data
		1       1       957     1       299.98  299.98
		2       2       1073    1       199.99  199.99    
		3       2       502     5       250.0   50.0      
		4       2       403     1       129.99  129.99    
		5       4       897     2       49.98   24.99     
		6       4       365     5       299.95  59.99     
		7       4       502     3       150.0   50.0      
		8       4       1014    4       199.92  49.98     
		9       5       957     1       299.98  299.98    
		10      5       365     5       299.95  59.99 

scenario:1 create a table YOUR_NAME_retail_db_ORC and load tables orders, order_items in your database.

When we create a database -> create database kazaparv_retail_db_orc;
use kazaparv_retail_db_orc;

- Now, lets create the tables

// create table orders (
     order_id int,
     order_date string,
     order_customer_id int,
     order_status string
     ) row format delimited fields terminated by ','
	 stored as orc;
		
// create table order_items (
     order_item_id int,
     order_item_order_id int,
     order_item_product_id int,
     order_item_quantity int,
	 order_item_subtotal float,
	 order_item_product_price float
     ) row format delimited fields terminated by ','
	 stored as orc;		
		

// As both the tables are created, we can get the extended file details
		hive> describe formatted orders;
		OK
		# col_name              data_type               comment
		order_id                int
		order_date              string                    
		order_customer_id       int                       
		order_status            string                    
		# Detailed Table Information                      
		Database:               kazaparv_retail_db_orc    
		Owner:                  kazaparv_gmail_com        
		CreateTime:             Sun Mar 04 19:18:14 UTC 2018
		LastAccessTime:         UNKNOWN                   
		Protect Mode:           None                      
		Retention:              0                         
		Location:               hdfs://cloudera.mettl.com:8020/user/hive/warehouse/kazaparv_retail_db_orc.db/orders
		Table Type:             MANAGED_TABLE             
		Table Parameters:                                 
				transient_lastDdlTime   1520191094        
		# Storage Information                             
		SerDe Library:          org.apache.hadoop.hive.ql.io.orc.OrcSerde
		InputFormat:            org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
		OutputFormat:           org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
		Compressed:             No                        
		Num Buckets:            -1                        
		Bucket Columns:         []                        
		Sort Columns:           []                        
		Storage Desc Params:                              
				field.delim             ,                 
				serialization.format    ,                 
		Time taken: 0.077 seconds, Fetched: 30 row(s)         
		
		
// Also we could check the metastore, as shown above
> dfs -ls /user/hive/warehouse/kazaparv_retail_db_orc.db/orders - and we should see no file formats

- Now, to load the in these tables, as we have textfile format to ORC, we have 2 step process. We need load the data in to a staging table and then insert in to the tables

// insert into table orders select * from kazaparv_retail_db_txt.orders;

- similarly, we can do 

// insert into table order_items select * from kazaparv_retail_db_txt.order_items;

- Now lets check the metastore
//dfs -ls /user/hive/warehouse/kazaparv_retail_db_orc.db/orders and you should see 
	hive> dfs -ls /user/hive/warehouse/kazaparv_retail_db_orc.db/orders;
	Found 1 items
	-rwxrwxrwt   3 kazaparv_gmail_com oozie     163094 2018-03-04 19:28 /user/hive/warehouse/kazaparv_retail_db_orc.db/orders/000000_0
		
- Now lets run the select query to confirm the data has been loaded
	hive> select * from orders limit 10;
	OK                                                
	1       2013-07-25 00:00:00.0   11599   CLOSED
	2       2013-07-25 00:00:00.0   256     PENDING_PAYMENT
	3       2013-07-25 00:00:00.0   12111   COMPLETE  
	4       2013-07-25 00:00:00.0   8827    CLOSED    
	5       2013-07-25 00:00:00.0   11318   COMPLETE  
	6       2013-07-25 00:00:00.0   7130    COMPLETE  
	7       2013-07-25 00:00:00.0   4530    COMPLETE  
	8       2013-07-25 00:00:00.0   2911    PROCESSING
	9       2013-07-25 00:00:00.0   5657    PENDING_PAYMENT
	10      2013-07-25 00:00:00.0   5648    PENDING_PAYMENT
	Time taken: 0.054 seconds, Fetched: 10 row(s)     
	hive> select * from order_items limit 10;         
	OK                                                
	1       1       957     1       299.98  299.98    
	2       2       1073    1       199.99  199.99    
	3       2       502     5       250.0   50.0      
	4       2       403     1       129.99  129.99    
	5       4       897     2       49.98   24.99     
	6       4       365     5       299.95  59.99     
	7       4       502     3       150.0   50.0      
	8       4       1014    4       199.92  49.98     
	9       5       957     1       299.98  299.98    
	10      5       365     5       299.95  59.99     
	Time taken: 0.075 seconds, Fetched: 10 row(s)  
			
		
	
- Now, we can run spark-sql or spark-shell and verify the sqlContext is available.

scala> sqlContext
	res1: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@59ea2a36
		
- to use the hive commands in sqlContext, we can use 

// sqlContext.sql("use kazaparv_retail_db_txt")
	res4: org.apache.spark.sql.DataFrame = [result: string]

// sqlContext.sql("show tables").show - which will return the dataframe
	res5: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]
	
	+-----------+-----------+
	|  tableName|isTemporary|
	+-----------+-----------+
	|order_items|      false|
	|     orders|      false|
	+-----------+-----------+		
		
// scala> sqlContext.sql("select * from orders").show

	+--------+--------------------+-----------------+---------------+
	|order_id|          order_date|order_customer_id|   order_status|
	+--------+--------------------+-----------------+---------------+
	|       1|2013-07-25 00:00:...|            11599|         CLOSED|
	|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|
	|       3|2013-07-25 00:00:...|            12111|       COMPLETE|
	|       4|2013-07-25 00:00:...|             8827|         CLOSED|
	|       5|2013-07-25 00:00:...|            11318|       COMPLETE|
	|       6|2013-07-25 00:00:...|             7130|       COMPLETE|
	|       7|2013-07-25 00:00:...|             4530|       COMPLETE|
	|       8|2013-07-25 00:00:...|             2911|     PROCESSING|
	|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|
	|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|
	|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|
	|      12|2013-07-25 00:00:...|             1837|         CLOSED|
	|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|
	|      14|2013-07-25 00:00:...|             9842|     PROCESSING|
	|      15|2013-07-25 00:00:...|             2568|       COMPLETE|
	|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|
	|      17|2013-07-25 00:00:...|             2667|       COMPLETE|
	|      18|2013-07-25 00:00:...|             1205|         CLOSED|
	|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|
	|      20|2013-07-25 00:00:...|             9198|     PROCESSING|
	+--------+--------------------+-----------------+---------------+
	only showing top 20 rows  		
		
- Note:, by default, it shows 20 records and you can limit the data as always if requried. There are some execptions, where we cant execute hive query in spark.However, 99% of the time, we should be comfortably exectute hive query's.

- understanding functions,
- Hive supports approx 200+ odd functions to perform operations easily. lets take length function for ex:

// select length('kazaparv');
	OK
	8
	Time taken: 0.683 seconds, Fetched: 1 row(s)

- To, apply the same lenght on a table columns, we could use the following

//select length(order_status) from orders limit 10;

	Query ID = kazaparv_gmail_com_20180304200202_8fd4f308-1873-4ec9-9b45-68cb1d8557e8
	Total jobs = 1                                                   
	Launching Job 1 out of 1                                         
	Number of reduce tasks is set to 0 since there's no reduce operator
	Starting Job = job_1518237550302_7217, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7217/
	Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7217
	Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
	2018-03-04 20:02:08,523 Stage-1 map = 0%,  reduce = 0%           
	2018-03-04 20:02:13,733 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.64 sec
	MapReduce Total cumulative CPU time: 2 seconds 640 msec          
	Ended Job = job_1518237550302_7217                               
	MapReduce Jobs Launched:                                         
	Stage-Stage-1: Map: 1   Cumulative CPU: 2.64 sec   HDFS Read: 69746 HDFS Write: 24 SUCCESS
	Total MapReduce CPU Time Spent: 2 seconds 640 msec               
	OK
	6                                                                
	15                                                               
	8                                                                
	6                                                                
	8                                                                
	8                                                                
	8                                                                
	10                                                               
	15                                                               
	15                                                               
	Time taken: 11.132 seconds, Fetched: 10 row(s)

//select order_status, length(order_status) from orders limit 10; -- will give both the column name and the length assoicated to it.

	Query ID = kazaparv_gmail_com_20180304200404_6b692a43-a333-495c-a038-19b56996bb48      
	Total jobs = 1
	Launching Job 1 out of 1           
	Number of reduce tasks is set to 0 since there's no reduce operator     
	Starting Job = job_1518237550302_7218, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7218/          
	Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7218            
	Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0        
	2018-03-04 20:04:14,061 Stage-1 map = 0%,  reduce = 0%           
	2018-03-04 20:04:19,273 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.63 sec      
	MapReduce Total cumulative CPU time: 2 seconds 630 msec          
	Ended Job = job_1518237550302_7218 
	MapReduce Jobs Launched:           
	Stage-Stage-1: Map: 1   Cumulative CPU: 2.63 sec   HDFS Read: 69889 HDFS Write: 133 SUCCESS           
	Total MapReduce CPU Time Spent: 2 seconds 630 msec
	OK
	CLOSED  6           
	PENDING_PAYMENT 15  
	COMPLETE        8   
	CLOSED  6           
	COMPLETE        8   
	COMPLETE        8   
	COMPLETE        8   
	PROCESSING      10  
	PENDING_PAYMENT 15  
	PENDING_PAYMENT 15  
	Time taken: 11.072 seconds, Fetched: 10 row(s)

// creating a table called customers and then load customers data

create table customers (
customer_id int,
customer_fname varchar(45),
customer_lname varchar(45), 
customer_email varchar(45),
customer_password varchar(45),
customer_street varchar(255),
customer_city varchar(45),
customer_state varchar(45),
customer_zipcode varchar(45)
) row format delimited fields terminated by ','
stored as textfile;

// load data local inpath '/home/kazaparv_gmail_com/HadoopInfoDataset-master/retail_db/customers/part-00000' into table customers

~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@# Important string functions~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#

* substr or substring
* instr
* like
* rlike
* lenght
* lcase or lower
* ucase or upper
* initcap
* trim, ltrim, rtrim
* cast,
* lpad, rpad
* split
* concat

So for substr or substring, the synatx is
// describe function substr
	OK
	substr(str, pos[, len]) - returns the substring of str that starts at pos and is of length len orsubstr(bin, pos[, len]) - returns the slice of byte array that starts at pos and
	 is of length len
	Time taken: 0.009 seconds, Fetched: 1 row(s)
	
// select ('I have two sons of age three and one');
	OK
	I have two sons of age three and one
	Time taken: 0.051 seconds, Fetched: 1 row(s)

	- Now to apply substr on above example
	
// select substr('I have two sons of age three and one',14); - to extract the substring from a postion based value
	OK
	ns of age three and one
	Time taken: 0.055 seconds, Fetched: 1 row(s)
	
// select substr('I have two sons of age three and one',1,16); - to extract part of string.
	OK
	I have two sons
	Time taken: 0.05 seconds, Fetched: 1 row(s)

// select substr('I have two sons of age three and one',-3); - to read last three characters
	OK
	one
	Time taken: 0.082 seconds, Fetched: 1 row(s)

// select substr('I have two sons of age three and one',-13,13); - to count from the 13th character from back of the string; till 13 characters.
	OK
	three and one
	Time taken: 0.056 seconds, Fetched: 1 row(s)
	
- Instr will provide the first occurance of the characters

// select instr('I have two sons of age three and one',' '); - will give the first occurance empty space in this example; should be '2' as per the examples
	OK
	2
	Time taken: 0.048 seconds, Fetched: 1 row(s)
	
//  select instr('I have two sons of age three and one','and'); - will give the first occurance of this word from string
	OK
	30
	Time taken: 0.061 seconds, Fetched: 1 row(s)
	
// select "I have two sons of age three and one" like '%one%'; - verifies if string contains the keyword 'one'
	OK
	true
	Time taken: 0.048 seconds, Fetched: 1 row(s)
	

	- to use regular expressions
// select "I have two sons of age three and one" rlike '[a-z][1-0]'; -- *** Dont use this *******


//select length("I have two sons of age three and one"); - will give length of the whole string
	OK
	36
	Time taken: 0.048 seconds, Fetched: 1 row(s)
	
// select lcase("I have two sons of age three and one"); - will convert to lower case
	OK
	i have two sons of age three and one
	Time taken: 0.054 seconds, Fetched: 1 row(s)
	
// select ucase("I have two sons of age three and one"); - will convert to upper case
	OK
	I HAVE TWO SONS OF AGE THREE AND ONE
	Time taken: 0.048 seconds, Fetched: 1 row(s)	

// select ucase("I have two sons of age three and one"); - will convert to upper case
	OK
	I HAVE TWO SONS OF AGE THREE AND ONE
	Time taken: 0.048 seconds, Fetched: 1 row(s)
	
// select initcap("I have two sons of age three and one"); - will convert every first letter with "upper case"
	OK
	I Have Two Sons Of Age Three And One
	Time taken: 0.072 seconds, Fetched: 1 row(s)
	
// select trim(" I have two sons of age three and one "), length(trim(" I have two sons of age three and one ")); - will remove leading and trailing spaces and gives len;
	OK
	I have two sons of age three and one    36
	Time taken: 0.07 seconds, Fetched: 1 row(s)
	
	- to trim right space you can use rtrim and if you wanted trim left spaces, then you could use ltrim
	
// select lpad(000012, 6, '8'); - ("string","length","paddingtype-with-required of padding")
	OK
	888812
	Time taken: 0.46 seconds, Fetched: 1 row(s)
	
	- Note, this is exculsively used, to replace '0'(s) in front of a number that doesnt have '0' (s) in it.
	
// select cast ("12" as int); - will convert to integer
	OK
	12
	Time taken: 0.076 seconds, Fetched: 1 row(s)
	
- now to cast the date format from the "orders" table from "kazaparv_retail_db_txt" database
//select cast(substr(order_date, 6,2) as int) from orders limit 10; - will cast the data to integer

	Query ID = kazaparv_gmail_com_20180304211717_70052aaa-c64e-494c-bae2-4de6db52bdb9
	Total jobs = 1
	Launching Job 1 out of 1  
	Number of reduce tasks is set to 0 since there's no reduce operator  
	Starting Job = job_1518237550302_7219, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7219/   
	Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7219 
	Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0  
	2018-03-04 21:17:40,477 Stage-1 map = 0%,  reduce = 0%   
	2018-03-04 21:17:45,663 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.75 sec
	MapReduce Total cumulative CPU time: 2 seconds 750 msec  
	Ended Job = job_1518237550302_7219
	MapReduce Jobs Launched:  
	Stage-Stage-1: Map: 1   Cumulative CPU: 2.75 sec   HDFS Read: 69887 HDFS Write: 20 SUCCESS   
	Total MapReduce CPU Time Spent: 2 seconds 750 msec
	OK
	7 
	7 
	7 
	7 
	7 
	7 
	7 
	7 
	7 
	7 
	Time taken: 10.981 seconds, Fetched: 10 row(s)
		
// select split("I have two sons of age three and one"," "); will split the data, based on the spaces

		Query ID = kazaparv_gmail_com_20180304212020_91395dc3-dbeb-43ff-b6e5-6496d08f6125     
		Total jobs = 1       
		Launching Job 1 out of 1          
		Number of reduce tasks is set to 0 since there's no reduce operator      
		Starting Job = job_1518237550302_7220, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7220/   
		Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7220     
		Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0       
		2018-03-04 21:20:57,655 Stage-1 map = 0%,  reduce = 0%      
		2018-03-04 21:21:01,911 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.62 sec     
		MapReduce Total cumulative CPU time: 2 seconds 620 msec     
		Ended Job = job_1518237550302_7220
		MapReduce Jobs Launched:          
		Stage-Stage-1: Map: 1   Cumulative CPU: 2.62 sec   HDFS Read: 4118 HDFS Write: 37 SUCCESS          
		Total MapReduce CPU Time Spent: 2 seconds 620 msec          
		OK      
		["I","have","two","sons","of","age","three","and","one"]    
		Time taken: 11.08 seconds, Fetched: 1 row(s)
		
- Now as the data is converted to list, to access list elements

// select index(split("I have two sons of age three and one"," "), 4); - will give me the 4th indexed value from list(of).

	Query ID = kazaparv_gmail_com_20180304212424_f2a6f120-c646-4e35-81ff-7fdb82f5b543      
	Total jobs = 1 
	Launching Job 1 out of 1
	Number of reduce tasks is set to 0 since there's no reduce operator  
	Starting Job = job_1518237550302_7221, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7221/ 
	Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7221   
	Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0        
	2018-03-04 21:24:08,700 Stage-1 map = 0%,  reduce = 0%      
	2018-03-04 21:24:12,858 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec      
	MapReduce Total cumulative CPU time: 2 seconds 560 msec     
	Ended Job = job_1518237550302_7221        
	MapReduce Jobs Launched:
	Stage-Stage-1: Map: 1   Cumulative CPU: 2.56 sec   HDFS Read: 4120 HDFS Write: 3 SUCCESS        
	Total MapReduce CPU Time Spent: 2 seconds 560 msec 
	OK    
	of    
	Time taken: 12.015 seconds, Fetched: 1 row(s)
!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#

- Important Date manupulation functions

* current_date
* current_timestamp
* date_add
* date_format
* date_sub
* datediff
* day
* dayofmonth
* to_date
* to_unix_timestamp
* to_utc_timestamp
* from_unixtime
* from_utc_timestamp
* minute
* month
* months_between
* next_day

// select current_date; - will give current date
	OK
	2018-03-04      
	Time taken: 0.076 seconds, Fetched: 1 row(s)

// select current_timestamp; - will give current timestamp
	OK
	2018-03-04 21:33:10.201
	Time taken: 0.051 seconds, Fetched: 1 row(s) 

// select date_format(current_date, 'y'); - will extract the year ( and we have lot of parameters, that can be passed- check Hive documents)
	OK
	2018     
	Time taken: 0.05 seconds, Fetched: 1 row(s)
	
// select day(current_timestamp); - will give the day in the month or you can use dayofmonth
	OK
	4 
	Time taken: 0.073 seconds, Fetched: 1 row(s)
	
// select to_date(current_timestamp); - will give the date and excludes timestamp
	OK
	2018-03-04      
	Time taken: 0.05 seconds, Fetched: 1 row(s)
	
// select to_unix_timestamp(current_timestamp); - will give a unique timestamp according to unix, for each second
	OK
	1520199975      
	Time taken: 0.074 seconds, Fetched: 1 row(s)  
	
	-Note: if you have 5 TB of data and converting the dateformat from sql in Hive will be an overhead. Thus we could use this unixtime stamp, that will convert the dateformat in to an long integer that reduces the conversion over head and increase faster processing time.
	
	- Converting the date and time, to unixtimestamp - from "orders" table from "kazaparv_retail_db_txt.db" database
	
// select order_date, to_unix_timestamp(order_date) from orders limit 10;
	
	OK
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	2013-07-25 00:00:00.0   1374710400   
	Time taken: 0.049 seconds, Fetched: 10 row(s) 
	
	-- Also to add 10 days for an exsisting date, we can use the following	
// select date_add((order_date),10) from orders limit 10;	

	Query ID = kazaparv_gmail_com_20180304220101_f136681f-64b6-42f6-b1a7-fe477bd8213d     
	Total jobs = 1  
	Launching Job 1 out of 1      
	Number of reduce tasks is set to 0 since there's no reduce operator     
	Starting Job = job_1518237550302_7222, Tracking URL = http://cloudera.mettl.com:8088/proxy/application_1518237550302_7222/      
	Kill Command = /opt/cloudera/parcels/CDH-5.13.1-1.cdh5.13.1.p0.2/lib/hadoop/bin/hadoop job  -kill job_1518237550302_7222 
	Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
	2018-03-04 22:02:00,376 Stage-1 map = 0%,  reduce = 0%    
	2018-03-04 22:02:05,564 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.73 sec     
	MapReduce Total cumulative CPU time: 2 seconds 730 msec   
	Ended Job = job_1518237550302_7222   
	MapReduce Jobs Launched:      
	Stage-Stage-1: Map: 1   Cumulative CPU: 2.73 sec   HDFS Read: 69842 HDFS Write: 110 SUCCESS  
	Total MapReduce CPU Time Spent: 2 seconds 730 msec 
	OK
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	2013-08-04      
	Time taken: 10.945 seconds, Fetched: 10 row(s)
	
// select from_unixtime(to_unix_timestamp(current_timestamp)); - will covert unique timestamp according to unix, for each second in to the system date timestamp
	OK
	1520199975      
	Time taken: 0.074 seconds, Fetched: 1 row(s) 

!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#

-- Important Aggregate functions

* count
* sum
* avg
* min
* max



// select count(*) from orders;
	OK
	68883    
	Time taken: 16.187 seconds, Fetched: 1 row(s)
	
	
// select sum(order_item_subtotal) from order_items;
	OK
	3.432262059842491E7    
	Time taken: 16.187 seconds, Fetched: 1 row(s) 
	
// select avg(order_item_subtotal) from order_items;
	OK
	199.32066922046081     
	Time taken: 16.086 seconds, Fetched: 1 row(s) 
	
// select max(order_item_subtotal) as maximum , min(order_item_subtotal) as minimum from order_items;
	OK
	1999.99 9.99    
	Time taken: 15.122 seconds, Fetched: 1 row(s)  
	-- Limitation: takes multiple records as i/p and one o/p;
	
!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#~!@#

-- Case: & NVL

Case statements are similiarly like if-else 
Syntax: CASE a WHEN b THEN c[WHEN d THEN e]* [ELSE f] END - when a = b, returns c; wnem a = d then returns e; else return false

// select distinct order_status from orders;
- scenario: if we wanted to take an action on all "completed orders" and then any "pending status" as another action, and if the order is "losed / cancelled", will have another category.

// select order_status, case order_status when 'CLOSED' then 'No Action' when 'COMPLETE' then 'No Action' end from orders limit 10;
	OK
	CLOSED  No Action      
	PENDING_PAYMENT NULL   
	COMPLETE No Action     
	CLOSED  No Action      
	COMPLETE No Action     
	COMPLETE No Action     
	COMPLETE No Action     
	PROCESSING      NULL   
	PENDING_PAYMENT NULL   
	PENDING_PAYMENT NULL   
	Time taken: 10.967 seconds, Fetched: 10 row(s)  
	
	OR the query can be simplified into
	
// select order_status, case when order_status  IN ('CLOSED', 'COMPLETE') then 'No Action' end from orders limit 10;
	OK
	CLOSED  No Action      
	PENDING_PAYMENT NULL   
	COMPLETE No Action     
	CLOSED  No Action      
	COMPLETE No Action     
	COMPLETE No Action     
	COMPLETE No Action     
	PROCESSING      NULL   
	PENDING_PAYMENT NULL   
	PENDING_PAYMENT NULL   
	Time taken: 10.974 seconds, Fetched: 10 row(s) 

-- NVL is a special function, that can be used to replace all NULL values / any specified values to default

//select distinct(nvl(order_status, 'Status_missing')) from orders limit 100;
	OK
	CANCELED 
	CLOSED   
	COMPLETE 
	ON_HOLD  
	PAYMENT_REVIEW  
	PENDING  
	PENDING_PAYMENT 
	PROCESSING      
	SUSPECTED_FRAUD 
	Time taken: 17.181 seconds, Fetched: 9 row(s) 

similar to 
// select case when order_status null then 'Status_missing' else order_status end from orders limit 100;


// Row level transformations ( data standardization(Fixing the standard data fromats), data cleansing(remove additional characters) - data ofsecation(get data in a secured way ex: last 4 digits of social's))


Scenario1: wanted to extract the month and date from orders table and type cast to int, we can use the ex:

// select cast(date_format(order_date, 'YYYYMM') as int) from orders limit 10;
	OK
	201307   
	201307   
	201307   
	201307   
	201307   
	201307   
	201307   
	201307   
	201307   
	201307   
	Time taken: 10.95 seconds, Fetched: 10 row(s)
	
	
	- Now to create joins you would need unique columns from both the datasets. In our example we would join both the customer and orders table

// select o.*,c.* from orders o,customers c where o.order_customer_id = c.customer_id limit 10;

	- Now, joining the data using keyword "Join"
	
// select o.*,c.* from orders o join customers c on o.order_customer_id = c.customer_id limit 10;

	- Now, joining the data using keyword "Left outerjoin"
	
// select o.*,c.* from orders o left outer join customers c on o.order_customer_id = c.customer_id limit 10;

	- Now, if you wanted to fetch only those customer numbers, that have been ordered customer if is null
	
// select o.*,c.* from orders o join customers c on o.order_customer_id = c.customer_id where o.order_customer_id is null limit 10;

	- Now, if you wanted to use "IN" clause, by bringing the customers who has not ordered
	
// select * from customers where customer_id not in (select distinct order_customer_id from orders) - only runs on certain versions of hive.

 - Aggregations: Extensive
 
 // select order_status,count(1) order_count from orders group by order_status;
 
	OK      
	CANCELED        1428        
	CLOSED          7556                
	COMPLETE        22899       
	ON_HOLD         3798                
	PAYMENT_REVIEW  729         
	PENDING         7610                
	PENDING_PAYMENT 15030       
	PROCESSING      8275        
	SUSPECTED_FRAUD 1558        
	Time taken: 16.187 seconds, Fetched: 9 row(s) 
 
	- Now, to get the revenue details for each order_id from orders and the corresponding order_items table

// select o.order_id, sum(order_item_subtotal) order_revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_id;

	- Now, to get the revenue details for each order_id from orders and the corresponding order_items table for only those order_items subtotal value is > 1000, order status in completed or closed
	
// select o.order_id, o.order_date, o.order_status, sum (oi.order_item_subtotal) as revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED') group by o.order_id,o.order_date, o.order_status having sum(oi.order_item_subtotal) >= 1000 limit 100;

	OK      
	5       2013-07-25 00:00:00.0   COMPLETE        1129.8600387573242  
	12      2013-07-25 00:00:00.0   CLOSED  1299.8700256347656          
	28      2013-07-25 00:00:00.0   COMPLETE        1159.9000129699707  
	62      2013-07-25 00:00:00.0   CLOSED  1149.9400329589844          
	107     2013-07-26 00:00:00.0   COMPLETE        1039.9500198364258  
	143     2013-07-26 00:00:00.0   COMPLETE        1139.9000244140625  
	171     2013-07-26 00:00:00.0   COMPLETE        1239.8700370788574  
	221     2013-07-26 00:00:00.0   CLOSED  1059.9100189208984          
	287     2013-07-26 00:00:00.0   COMPLETE        1089.8900260925293  
	304     2013-07-26 00:00:00.0   COMPLETE        1019.900032043457   
	311     2013-07-26 00:00:00.0   COMPLETE        1045.8100204467773  
	338     2013-07-26 00:00:00.0   COMPLETE        1099.8600158691406  
	351     2013-07-27 00:00:00.0   COMPLETE        1043.8400192260742  
	384     2013-07-27 00:00:00.0   CLOSED  1039.9000244140625          
	403     2013-07-27 00:00:00.0   COMPLETE        1029.8600158691406  
	404     2013-07-27 00:00:00.0   CLOSED  1209.8300132751465          
	442     2013-07-27 00:00:00.0   COMPLETE        1229.910011291504   
	493     2013-07-27 00:00:00.0   COMPLETE        1129.9400329589844  
	599     2013-07-28 00:00:00.0   COMPLETE        1024.9300231933594  
	614     2013-07-28 00:00:00.0   CLOSED  1499.8700408935547  
	
	- Now, lets compute the total revenue based on the day
	
// select o.order_id, o.order_date, o.order_status, round(sum (oi.order_item_subtotal),2) as revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED') group by o.order_id,o.order_date, o.order_status limit 100;

	OK
	255     2013-07-26 00:00:00.0   COMPLETE        879.8099975585938   
	256     2013-07-26 00:00:00.0   CLOSED  589.9100151062012           
	259     2013-07-26 00:00:00.0   COMPLETE        409.9400100708008   
	260     2013-07-26 00:00:00.0   CLOSED  449.97999572753906          
	262     2013-07-26 00:00:00.0   CLOSED  879.8600158691406           
	263     2013-07-26 00:00:00.0   COMPLETE        100.0               
	264     2013-07-26 00:00:00.0   COMPLETE        899.9200286865234   
	266     2013-07-26 00:00:00.0   COMPLETE        199.99000549316406  
	267     2013-07-26 00:00:00.0   CLOSED  299.9700012207031           
	268     2013-07-26 00:00:00.0   CLOSED  239.96000289916992          
	271     2013-07-26 00:00:00.0   COMPLETE        889.870002746582    
	272     2013-07-26 00:00:00.0   CLOSED  191.9600067138672           
	279     2013-07-26 00:00:00.0   COMPLETE        199.97999572753906  
	283     2013-07-26 00:00:00.0   COMPLETE        569.8600044250488   
	285     2013-07-26 00:00:00.0   CLOSED  329.9800109863281           
	Time taken: 20.8 seconds, Fetched: 100 row(s)  
	
	
- Sorting the data can be using "order by" keyword

// select o.order_id, o.order_date, o.order_status, sum (oi.order_item_subtotal) as revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED') group by o.order_id,o.order_date, o.order_status having sum(oi.order_item_subtotal) >= 1000 order by o.order_id, revenue desc limit 100;

	OK      
	5       2013-07-25 00:00:00.0   COMPLETE        1129.8600387573242      
	12      2013-07-25 00:00:00.0   CLOSED  1299.8700256347656    
	28      2013-07-25 00:00:00.0   COMPLETE        1159.9000129699707      
	62      2013-07-25 00:00:00.0   CLOSED  1149.9400329589844    
	107     2013-07-26 00:00:00.0   COMPLETE        1039.9500198364258      
	143     2013-07-26 00:00:00.0   COMPLETE        1139.9000244140625   
	2301    2013-08-06 00:00:00.0   CLOSED  1019.830020904541     
	2326    2013-08-06 00:00:00.0   COMPLETE        1089.9200286865234      
	2337    2013-08-06 00:00:00.0   CLOSED  1129.8000183105469    
	2346    2013-08-06 00:00:00.0   COMPLETE        1219.8900146484375      
	2352    2013-08-06 00:00:00.0   COMPLETE        1209.9100341796875      
	2387    2013-08-06 00:00:00.0   COMPLETE        1029.890037536621       
	2396    2013-08-06 00:00:00.0   CLOSED  1329.920036315918     
	2427    2013-08-07 00:00:00.0   CLOSED  1119.8300323486328    
	Time taken: 36.994 seconds, Fetched: 100 row(s)
	
	- Now using distibuted by & sort by keywords will avoid sorting the data globally

// select o.order_id, o.order_date, o.order_status, sum (oi.order_item_subtotal) as revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED') group by o.order_id,o.order_date, o.order_status having sum(oi.order_item_subtotal) >= 1000 distribute by o.order_date sort by o.order_date, revenue desc limit 100;

	OK
	1582    2013-08-02 00:00:00.0   COMPLETE        1315.8400344848633  
	1654    2013-08-02 00:00:00.0   COMPLETE        1219.8500213623047  
	1676    2013-08-02 00:00:00.0   COMPLETE        1164.9100341796875  
	1571    2013-08-02 00:00:00.0   COMPLETE        1129.9000244140625  
	1607    2013-08-02 00:00:00.0   COMPLETE        1009.8800354003906  
	1800    2013-08-03 00:00:00.0   COMPLETE        1569.7900085449219  
	1788    2013-08-03 00:00:00.0   COMPLETE        1529.8100280761719  
	58004   2013-08-03 00:00:00.0   CLOSED  1278.9200286865234
	1858    2013-08-03 00:00:00.0   COMPLETE        1249.8500213623047  
	1865    2013-08-03 00:00:00.0   COMPLETE        1159.8600234985352  
	1745    2013-08-03 00:00:00.0   COMPLETE        1129.8700332641602  
	67454   2013-08-03 00:00:00.0   COMPLETE        1119.9400100708008  
	1830    2013-08-03 00:00:00.0   COMPLETE        1099.910026550293   
	1799    2013-08-03 00:00:00.0   COMPLETE        1079.9400253295898  
	57986   2013-08-03 00:00:00.0   COMPLETE        1049.9500274658203  
	1733    2013-08-03 00:00:00.0   COMPLETE        1049.8800315856934  
	Time taken: 53.991 seconds, Fetched: 100 row(s)
	
	- Now, using set operations
		-We can only use similar datasets that can support only union (Eliminate any duplicates) and union All.
		select 1, "Hello" 
		select 2, "World" 
		select 1, "Hello" 
		select 1, "Hey" 
		
	By using union all
	
//		select 1, "Hello" 
		union all
		select 2, "World" 
		union all
		select 1, "Hello" 
		union all
		select 1, "Hey"; 
		
		
		OK      
		1       Hello     
		2       World     
		1       Hello     
		1       Hey       
		Time taken: 11.071 seconds, Fetched: 4 row(s)   
		
			
		
//	By using union 
	
		select 1, "Hello" 
		union
		select 2, "World" 
		union
		select 1, "Hello" 
		union
		select 1, "Hey";
		
		
		OK      
		1       Hello     
		2       World      
		1       Hey       
		Time taken: 11.071 seconds, Fetched: 4 row(s)  
		
		- generally analytics function used by a keyword "over"  on top of aggregations such as 
			- sum,min,max,count,avg
			- over with a "partition by" statement with one or more columns
		
scenario 1: select, order id, order date, order status,order_subtotal, cummilative revenue and % of revenue generate with in each order.

// select * from ( 
select o.order_id, o.order_date, o.order_status,oi.order_item_subtotal, round(sum (oi.order_item_subtotal) over (partition by o.order_id), 2)  revenue, oi.order_item_subtotal/round(sum (oi.order_item_subtotal) over(partition by o.order_id),2)  perct, round(avg(oi.order_item_subtotal) over (partition by o.order_id),2) avg_perct from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED')) q 
where revenue >=  1000 order by order_date,revenue desc limit 100;

		OK
		57779   2013-07-25 00:00:00.0   COMPLETE        149.94  1649.8  0.0908837449638782      329.96             
		57779   2013-07-25 00:00:00.0   COMPLETE        299.98  1649.8  0.181828107034991       329.96             
		57779   2013-07-25 00:00:00.0   COMPLETE        399.98  1649.8  0.24244151472077108     329.96             
		57779   2013-07-25 00:00:00.0   COMPLETE        499.95  1649.8  0.3030367391241552      329.96             
		57779   2013-07-25 00:00:00.0   COMPLETE        299.95  1649.8  0.18180992375259503     329.96             
		12      2013-07-25 00:00:00.0   CLOSED  		149.94  1299.87 0.11534999841630798     259.97      
		12      2013-07-25 00:00:00.0   CLOSED  		100.0   1299.87 0.07693077000007693     259.97      
		12      2013-07-25 00:00:00.0   CLOSED  		250.0   1299.87 0.19232692500019236     259.97      
		12      2013-07-25 00:00:00.0   CLOSED  		499.95  1299.87 0.38461539400634775     259.97      
		12      2013-07-25 00:00:00.0   CLOSED  		299.98  1299.87 0.23077693229809762     259.97   
		Time taken: 0.071 seconds, Fetched: 100 row(s)  


// Analytical function - Raking. ( number assigned by a key with in the partition). Rank will help us to classify the TOP N values with certain inputs to it.
		- Also, rank function, will not generate, if Order by clause is not used.		
		- Now, we are getting the order id's for top revenue generated with in each order id based on Rank function- Tranking will be skipped if duplicates found
		- getting dense rank - will still assign rank for the duplicates found
		- getting the row number with & with out order by clause - each and everyitem in a group will be assigned with row number randomly, 
			if order_id doesnt 	exsits.
		- getting the % rank, based on the number order_items, it divides the % of order_tems
			ex: total of order_id are "5" then % will 1/5,2/5,3/5,4/5,5/5 --> 0.2,0.4,0.6,0.8 & 1.0 for a single order_id
		
Scenario2: Based on the above query, we trying to apply rank function, to allow to sort for the data based on getting the highest revenue for each order id and the sort the data by desc

// select * from ( 
select o.order_id, o.order_date, o.order_status,oi.order_item_subtotal, round(sum (oi.order_item_subtotal) over (partition by o.order_id), 2)  revenue, oi.order_item_subtotal/round(sum (oi.order_item_subtotal) over(partition by o.order_id),2)  perct, round(avg(oi.order_item_subtotal) over (partition by o.order_id),2) avg_perct,rank() over(partition by o.order_id order by oi.order_item_subtotal desc) rnk_revenue,dense_rank() over(partition by o.order_id order by oi.order_item_subtotal desc)  dense_rank_revenue,row_number() over(partition by o.order_id order by oi.order_item_subtotal desc)  rnk_order_by_revenue,PERCENT_RANK() over(partition by o.order_id order by oi.order_item_subtotal desc)  pct_revenue, row_number() over(partition by o.order_id)  rnk_revenue_wo_orderby from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED')) q 
where revenue >=  1000 order by order_date,revenue desc limit 100;


		OK   
		57779   2013-07-25 00:00:00.0   COMPLETE        299.95  1649.8  0.18180992375259503     329.96  4       4       4       0.75    3  
		57779   2013-07-25 00:00:00.0   COMPLETE        499.95  1649.8  0.3030367391241552      329.96  1       1       1       0.0     5  
		57779   2013-07-25 00:00:00.0   COMPLETE        399.98  1649.8  0.24244151472077108     329.96  2       2       2       0.25    2  
		57779   2013-07-25 00:00:00.0   COMPLETE        149.94  1649.8  0.0908837449638782      329.96  5       5       5       1.0     4  
		57779   2013-07-25 00:00:00.0   COMPLETE        299.98  1649.8  0.181828107034991       329.96  3       3       3       0.5     1  
		12      2013-07-25 00:00:00.0   CLOSED  		250.0   1299.87 0.19232692500019236     259.97  3       3       3       0.5     5          
		12      2013-07-25 00:00:00.0   CLOSED  		299.98  1299.87 0.23077693229809762     259.97  2       2       2       0.25    1          
		12      2013-07-25 00:00:00.0   CLOSED  		100.0   1299.87 0.07693077000007693     259.97  5       5       5       1.0     2          
		12      2013-07-25 00:00:00.0   CLOSED  		149.94  1299.87 0.11534999841630798     259.97  4       4       4       0.75    3          
		12      2013-07-25 00:00:00.0   CLOSED  		499.95  1299.87 0.38461539400634775     259.97  1       1       1       0.0     4          
		28      2013-07-25 00:00:00.0   COMPLETE        99.99   1159.9  0.08620570554683121     231.98  4       3       4       0.75    5  
		28      2013-07-25 00:00:00.0   COMPLETE        399.96  1159.9  0.34482282218732485     231.98  1       1       1       0.0     2  
		28      2013-07-25 00:00:00.0   COMPLETE        59.99   1159.9  0.05171997730706681     231.98  5       4       5       1.0     1  
		28      2013-07-25 00:00:00.0   COMPLETE        299.98  1159.9  0.2586257530703751      231.98  2       2       2       0.25    3  
		28      2013-07-25 00:00:00.0   COMPLETE        299.98  1159.9  0.2586257530703751      231.98  2       2       3       0.25    4  
		62      2013-07-25 00:00:00.0   CLOSED  		399.98  1149.94 0.3478268526934693      287.49  1       1       1       0.0     1          
		62      2013-07-25 00:00:00.0   CLOSED  		399.98  1149.94 0.3478268526934693      287.49  1       1       2       0.0     2          
		62      2013-07-25 00:00:00.0   CLOSED 			299.98  1149.94 0.2608657938556169      287.49  3       2       3       0.6666666666666666      3
		62      2013-07-25 00:00:00.0   CLOSED  		50.0    1149.94 0.0434805294189262      287.49  4       3       4       1.0     4                
		Time taken: 54.972 seconds, Fetched: 100 row(s)
		
		- LEAD: fetch the next record in the group ( paginantion is best example ">" in a webpage to sort the pages)
		- LAG: fect the previous record in the group "<"
		- FIRST_VALUE: Fetch the first record in the group "<<"
		- LAST_VAUE: fetch the last record in the group ">>"
		
		

// select * from ( 
select o.order_id, o.order_date, o.order_status,oi.order_item_subtotal, 
round(sum (oi.order_item_subtotal) over (partition by o.order_id), 2)  revenue, 
oi.order_item_subtotal/round(sum (oi.order_item_subtotal) over(partition by o.order_id),2)  perct, 
round(avg(oi.order_item_subtotal) over (partition by o.order_id),2) avg_perct,
rank() over(partition by o.order_id order by oi.order_item_subtotal desc) rnk_revenue,
dense_rank() over(partition by o.order_id order by oi.order_item_subtotal desc)  dense_rank_revenue,
PERCENT_RANK() over(partition by o.order_id order by oi.order_item_subtotal desc)  pct_revenue,
row_number() over(partition by o.order_id order by oi.order_item_subtotal desc)  rnk_order_by_revenue,
row_number() over(partition by o.order_id)  rnk_revenue_wo_orderby,
lead(oi.order_item_subtotal) over(partition by o.order_id)  Lead_order_item_subtotal,
lag(oi.order_item_subtotal) over(partition by o.order_id)  Lag_order_item_subtotal,
first_value(oi.order_item_subtotal) over(partition by o.order_id)  first_order_item_subtotal,
last_value(oi.order_item_subtotal) over(partition by o.order_id)  last_order_item_subtotal
from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status IN('COMPLETE', 'CLOSED')) q 
where revenue >=  1000 order by order_date,revenue desc limit 100;

OK

311     2013-07-26 00:00:00.0   COMPLETE        149.94  1045.81 0.14337212537784708     209.16  4       4       0.75    4       2       399.98  159.95  159.95  95.98   
311     2013-07-26 00:00:00.0   COMPLETE        239.96  1045.81 0.22944895030059687     209.16  2       2       0.25    2       4       95.98   399.98  159.95  95.98   
311     2013-07-26 00:00:00.0   COMPLETE        399.98  1045.81 0.3824595394826289      209.16  1       1       0.0     1       3       239.96  149.94  159.95  95.98   
311     2013-07-26 00:00:00.0   COMPLETE        159.95  1045.81 0.15294364841437946     209.16  3       3       0.5     3       1       149.94  NULL    159.95  95.98   
311     2013-07-26 00:00:00.0   COMPLETE        95.98   1045.81 0.09177575597568736     209.16  5       5       1.0     5       5       NULL    239.96  159.95  95.98   
107     2013-07-26 00:00:00.0   COMPLETE        299.98  1039.95 0.28845618634196657     207.99  1       1       0.0     1       5       NULL    129.99  79.98   299.98  
107     2013-07-26 00:00:00.0   COMPLETE        79.98   1039.95 0.07690754685988133     207.99  5       5       1.0     5       1       280.0   NULL    79.98   299.98  
107     2013-07-26 00:00:00.0   COMPLETE        280.0   1039.95 0.2692437136400788      207.99  2       2       0.25    2       2       250.0   79.98   79.98   299.98  
107     2013-07-26 00:00:00.0   COMPLETE        250.0   1039.95 0.24039617289292753     207.99  3       3       0.5     3       3       129.99  280.0   79.98   299.98  
107     2013-07-26 00:00:00.0   COMPLETE        129.99  1039.95 0.12499639933954908     207.99  4       4       0.75    4       4       299.98  250.0   79.98   299.98  
68692   2013-07-26 00:00:00.0   COMPLETE        199.99  1029.94 0.1941763651214285      205.99  2       2       0.25    2       2       399.98  129.99  129.99  100.0   
68692   2013-07-26 00:00:00.0   COMPLETE        100.0   1029.94 0.09709303454570169     205.99  5       5       1.0     5       5       NULL    199.98  129.99  100.0   
68692   2013-07-26 00:00:00.0   COMPLETE        399.98  1029.94 0.388352730242857       205.99  1       1       0.0     1       3       199.98  199.99  129.99  100.0   
68692   2013-07-26 00:00:00.0   COMPLETE        199.98  1029.94 0.19416664633623226     205.99  3       3       0.5     3       4       100.0   399.98  129.99  100.0   
68692   2013-07-26 00:00:00.0   COMPLETE        129.99  1029.94 0.1262112409394373      205.99  4       4       0.75    4       1       199.99  NULL    129.99  100.0   
304     2013-07-26 00:00:00.0   COMPLETE        399.98  1019.9  0.3921757142723092      254.98  1       1       0.0     1       2       119.98  299.95  299.95  199.99  
304     2013-07-26 00:00:00.0   COMPLETE        299.95  1019.9  0.2940974725041977      254.98  2       2       0.3333333333333333      2       1       399.98  NULL    
299.95  199.99                                                                                                                                                          
304     2013-07-26 00:00:00.0   COMPLETE        199.99  1019.9  0.1960878571361546      254.98  3       3       0.6666666666666666      3       4       NULL    119.98  
299.95  199.99                                                                                                                                                          
304     2013-07-26 00:00:00.0   COMPLETE        119.98  1019.9  0.1176389875055727      254.98  4       4       1.0     4       3       199.99  399.98  299.95  199.99  
57812   2013-07-27 00:00:00.0   CLOSED  399.98  1259.93 0.31746208994652725     251.99  1       1       0.0     1       5       NULL    129.99  129.99  399.98          
57812   2013-07-27 00:00:00.0   CLOSED  399.98  1259.93 0.31746208994652725     251.99  1       1       0.0     2       2       199.99  129.99  129.99  399.98          
57812   2013-07-27 00:00:00.0   CLOSED  129.99  1259.93 0.1031724028264777      251.99  4       3       0.75    5       1       399.98  NULL    129.99  399.98          
57812   2013-07-27 00:00:00.0   CLOSED  129.99  1259.93 0.1031724028264777      251.99  4       3       0.75    4       4       399.98  199.99  129.99  399.98          
57812   2013-07-27 00:00:00.0   CLOSED  199.99  1259.93 0.15873104497326362     251.99  3       2       0.5     3       3       129.99  399.98  129.99  399.98          
442     2013-07-27 00:00:00.0   COMPLETE        79.98   1229.91 0.06502915120369261     245.98  5       5       1.0     5       5       NULL    399.96  199.99  79.98   
442     2013-07-27 00:00:00.0   COMPLETE        199.99  1229.91 0.16260539835692372     245.98  4       4       0.75    4       1       299.98  NULL    199.99  79.98   
Time taken: 58.15 seconds, Fetched: 100 row(s)


// Scenario3:

	- Create Spark dataframe
	- Perform operations on dataframe
	- Write a spark SQL application
	- use Hive ORC from spark SQLContext
	- Write a Spark SQL application that reads and writes data from Hive tables
	
	

	- Problem_Statement: Get daily revenue by product considering completed and closed orders
		- product has to be read from local files system. Data frame need to be created
		- join orders and order items
		- filter order_status
		- data has to be sorted asc order by desc order by revenue,computed for each product each day
			- sort data by order_date in ascending order and then daily revenue per product in desc

// creating an spark dataframe
	-- create a spark RDD file
// var sporders = sc.textFile("/user/kazaparv_gmail_com/Orders") 
	-- Convert the RDD using a dataframe
// var orderDF = sporders.map( rec => (rec.split(",")(0).toInt,rec.split(",")(1),rec.split(",")(2).toInt,rec.split(",")(3))).toDF 
	-- Show details in RDD
// var orderDF = sporders.map( rec => (rec.split(",")(0).toInt,rec.split(",")(1),rec.split(",")(2).toInt,rec.split(",")(3))).toDF("order_id","order_date","order_customer_id","order_status").show()	
		+--------+--------------------+-----------------+---------------+             
		|order_id|          order_date|order_customer_id|   order_status|             
		+--------+--------------------+-----------------+---------------+             
		|       1|2013-07-25 00:00:...|            11599|         CLOSED|             
		|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|             
		|       3|2013-07-25 00:00:...|            12111|       COMPLETE|             
		|       4|2013-07-25 00:00:...|             8827|         CLOSED|             
		|       5|2013-07-25 00:00:...|            11318|       COMPLETE|             
		|       6|2013-07-25 00:00:...|             7130|       COMPLETE|             
		|       7|2013-07-25 00:00:...|             4530|       COMPLETE|             
		|       8|2013-07-25 00:00:...|             2911|     PROCESSING|             
		|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|             
		|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|             
		|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|             
		|      12|2013-07-25 00:00:...|             1837|         CLOSED|             
		|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|             
		|      14|2013-07-25 00:00:...|             9842|     PROCESSING|             
		|      15|2013-07-25 00:00:...|             2568|       COMPLETE|             
		|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|             
		|      17|2013-07-25 00:00:...|             2667|       COMPLETE|             
		|      18|2013-07-25 00:00:...|             1205|         CLOSED|             
		|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|             
		|      20|2013-07-25 00:00:...|             9198|     PROCESSING|             
		+--------+--------------------+-----------------+---------------+

//orderDF.printSchema

		root
		 |-- order_id: integer (nullable = false)             
		 |-- order_date: string (nullable = true)             
		 |-- order_customer_id: integer (nullable = false)    
		 |-- order_status: string (nullable = true)           
-- Now lets create a temp table, to run querys against this table

//orderDF.registerTempTable("orders")

-- Now verify the temp table

 // sqlContext.sql("select * from orders").show()
 // sqlContext.sql("select order_status,count(1) countby_status from orders group by order_status").show()
 
	+---------------+--------------+
	|   order_status|countby_status|    
	+---------------+--------------+    
	|        PENDING|          7610|    
	|        ON_HOLD|          3798|    
	| PAYMENT_REVIEW|           729|    
	|PENDING_PAYMENT|         15030|    
	|     PROCESSING|          8275|    
	|         CLOSED|          7556|    
	|       COMPLETE|         22899|    
	|       CANCELED|          1428|    
	|SUSPECTED_FRAUD|          1558|    
	+---------------+--------------+ 

-- Now load the data from the local file system

// val RawProducts = scala.io.SourcefromFile("/user/kazaparv_gmail_com/products/part-00000").getLines()
// or val ProductsRDD = sc.textFile("/user/kazaparv_gmail_com/products")

	-- Now the RDD created. Lets create the dataframe
	
//val productsDF = ProductsRDD.map (rec => (rec.split(",")(0).toInt,rec.split(",")(2))).toDF("product_id","product_name")

	+----------+--------------------+   
	|product_id|        product_name|   
	+----------+--------------------+   
	|         1|Quest Q64 10 FT. ...|   
	|         2|Under Armour Men'...|   
	|         3|Under Armour Men'...|   
	|         4|Under Armour Men'...|   
	|         5|Riddell Youth Rev...|   
	|         6|Jordan Men's VI R...|   
	|         7|Schutt Youth Recr...|   
	|         8|Nike Men's Vapor ...|   
	|         9|Nike Adult Vapor ...|   
	|        10|Under Armour Men'...|   
	|        11|Fitness Gear 300 ...|   
	|        12|Under Armour Men'...|   
	|        13|Under Armour Men'...|   
	|        14|Quik Shade Summit...|   
	|        15|Under Armour Kids...|   
	|        16|Riddell Youth 360...|   
	|        17|Under Armour Men'...|   
	|        18|Reebok Men's Full...|   
	|        19|Nike Men's Finger...|   
	|        20|Under Armour Men'...|   
	+----------+--------------------+   
	only showing top 20 rows
  
	-- Now lets create temp table
	
// productsDF.registerTempTable("products")
// sqlContext.sql("select * from products").show()


similiarly let us do it for order_items

/ creating an spark dataframe
	-- create a spark RDD file
// var spordersitems = sc.textFile("/user/kazaparv_gmail_com/order_items") 
	-- Convert the RDD using a dataframe
	order_item_id int,
     order_item_order_id int,
     order_item_product_id int,
     order_item_quantity int,
	 order_item_subtotal float,
	 order_item_product_price float
	-- Show details in RDD
	
// var ordersitemsDF = spordersitems.map( rec => (rec.split(",")(0).toInt,rec.split(",")(1).toInt,rec.split(",")(2).toInt,rec.split(",")(3).toInt,rec.split(",")(4).toFloat,rec.split(",")(5).toFloat)).toDF("order_item_id","order_item_order_id","order_item_product_id","order_item_quantity","order_item_subtotal","order_item_product_price")	

//ordersitemsDF.registerTempTable("order_items")

//sqlContext.sql("select * from order_items").show()

-------------+-------------------+---------------------+-------------------+-------------------+------------------------+  
|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|  
+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+  
|            1|                  1|                  957|                  1|             299.98|                  299.98|  
|            2|                  2|                 1073|                  1|             199.99|                  199.99|  
|            3|                  2|                  502|                  5|              250.0|                    50.0|  
|            4|                  2|                  403|                  1|             129.99|                  129.99|  
|            5|                  4|                  897|                  2|              49.98|                   24.99|  
|            6|                  4|                  365|                  5|             299.95|                   59.99|  
|            7|                  4|                  502|                  3|              150.0|                    50.0|  
|            8|                  4|                 1014|                  4|             199.92|                   49.98|  
|            9|                  5|                  957|                  1|             299.98|                  299.98|  
|           10|                  5|                  365|                  5|             299.95|                   59.99|  
|           11|                  5|                 1014|                  2|              99.96|                   49.98|  
|           12|                  5|                  957|                  1|             299.98|                  299.98|  
|           13|                  5|                  403|                  1|             129.99|                  129.99|  
|           14|                  7|                 1073|                  1|             199.99|                  199.99|  
|           15|                  7|                  957|                  1|             299.98|                  299.98|  
|           16|                  7|                  926|                  5|              79.95|                   15.99|  
|           17|                  8|                  365|                  3|             179.97|                   59.99|  
|           18|                  8|                  365|                  5|             299.95|                   59.99|  
|           19|                  8|                 1014|                  4|             199.92|                   49.98|  
|           20|                  8|                  502|                  1|               50.0|                    50.0|  
+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+  
only showing top 20 rows     


-- Now,
	- Problem_Statement: Get daily revenue by product considering completed and closed orders
		- product has to be read from local files system. Data frame need to be created
		- join orders and order items
		- filter order_status
		- data has to be sorted asc order by desc order by revenue,computed for each product each day
			- sort data by order_date in ascending order and then daily revenue per product in desc
			
// sqlContext.sql("select o.order_date,p.product_name, sum(oi.order_item_subtotal) revenue_per
from orders o join order_items oi ON o.order_id = oi.order_item_id 
join products p ON p.product_id = oi.order_item_product_id
where o.order_status IN ('COMPLETE' , 'CLOSED')
group by o.order_date,p.product_name
order by order_date, revenue_per desc").show

+--------------------+--------------------+------------------+
|          order_date|        product_name|       revenue_per|                  
+--------------------+--------------------+------------------+                  
|2013-07-25 00:00:...|Pelican Sunstream...|1599.9200439453125|                  
|2013-07-25 00:00:...|Nike Men's Dri-FI...|            1550.0|                  
|2013-07-25 00:00:...|Nike Men's CJ Eli...|1429.8900604248047|                  
|2013-07-25 00:00:...|Perfect Fitness P...|1259.7900428771973|                  
|2013-07-25 00:00:...|Field & Stream Sp...|1199.9400329589844|                  
|2013-07-25 00:00:...|Diamondback Women...|1199.9200439453125|                  
|2013-07-25 00:00:...|Nike Men's Free 5...| 999.9000015258789|                  
|2013-07-25 00:00:...|O'Brien Men's Neo...| 799.6799926757812|                  
|2013-07-25 00:00:...|Under Armour Girl...| 439.8900146484375|                  
|2013-07-25 00:00:...|Under Armour Wome...|127.95999908447266|                  
|2013-07-25 00:00:...|Nike Women's Lege...|             100.0|                  
|2013-07-25 00:00:...|Team Golf St. Lou...| 99.95999908447266|                  
|2013-07-25 00:00:...|Bridgestone e6 St...| 95.97000122070312|                  
|2013-07-25 00:00:...|Team Golf Texas L...| 74.97000122070312|                  
|2013-07-25 00:00:...|Team Golf New Eng...| 49.97999954223633|                  
|2013-07-26 00:00:...|Field & Stream Sp...| 4399.780120849609|                  
|2013-07-26 00:00:...|Perfect Fitness P...|3779.3701095581055|                  
|2013-07-26 00:00:...|Diamondback Women...| 2099.860076904297|                  
|2013-07-26 00:00:...|O'Brien Men's Neo...|2099.1599731445312|                  
|2013-07-26 00:00:...|Nike Men's CJ Eli...| 2079.840087890625|                  
+--------------------+--------------------+------------------+                  
only showing top 20 rows 


	-- if there more number of task trackers  being used, you can use, "sqlContext.sql(spark.sql.shuffle.partitions","2")" to control tasks
	-- Now to write the data back to hive database under YOUR_USER_ID_daily_revenue
	-- Now, get the order date,product_name,daily_revenue_per_product and save the data in hive table using ORC file format.
	
	
	-- Now create database with the name
	
// sqlContext.sql("CREATE DATABASE kazaparv_daily_revenue")
//sqlContext.sql("CREATE TABLE kazaparv_daily_revenue.daily_revenue (order_date string, product_name string,daily_revenue_per_product float) STORED AS orc");

	-- Now , store the query o/p into a variable
	
// val daily_revenue = sqlContext.sql("select o.order_date,p.product_name, sum(oi.order_item_subtotal) revenue_per
from orders o join order_items oi ON o.order_id = oi.order_item_id 
join products p ON p.product_id = oi.order_item_product_id
where o.order_status IN ('COMPLETE' , 'CLOSED')
group by o.order_date,p.product_name
order by order_date, revenue_per desc")

	-- Now, you will need to insert the o/p obtained in the variable "daily_revenue" in to the created table
	-- varname insertInto("Database_Name.Table_Name");
	
	
// daily_revenue.insertInto("kazaparv_daily_revenue.daily_revenue")
// sqlContext.sql("select * from kazaparv_daily_revenue.daily_revenue limit 10").show

	+--------------------+--------------------+-------------------------+
	|          order_date|        product_name|daily_revenue_per_product|                   
	+--------------------+--------------------+-------------------------+                   
	|2013-07-25 00:00:...|Pelican Sunstream...|                  1599.92|                   
	|2013-07-25 00:00:...|Nike Men's Dri-FI...|                   1550.0|                   
	|2013-07-25 00:00:...|Nike Men's CJ Eli...|                  1429.89|                   
	|2013-07-25 00:00:...|Perfect Fitness P...|                  1259.79|                   
	|2013-07-25 00:00:...|Field & Stream Sp...|                1199.9401|                   
	|2013-07-25 00:00:...|Diamondback Women...|                  1199.92|                   
	|2013-07-25 00:00:...|Nike Men's Free 5...|		       		   999.9|
	|2013-07-25 00:00:...|O'Brien Men's Neo...|                   799.68|                   
	|2013-07-25 00:00:...|Under Armour Girl...|                   439.89|                   
	|2013-07-25 00:00:...|Under Armour Wome...|                   127.96|                   
	+--------------------+--------------------+-------------------------+  
	
	-- Dataframe operations

-- We can save the data back to hdfs by saying

// daily_revenue.save("/user/kazaparv_gmail_com/daily_revenue","json")
or 

// daily_revenue.write.json("/user/kazaparv_gmail_com/daily_revenue")

you can also convert the dataframe in to RDD 

//daily_revenue.rdd
//daily_revenue.rdd.take(10).foreach(println)

	-- if you wanted to pull only selected fields from the dataframe

//daily_revenue.select("order_date").show

	-- if you wanted to count & filter the data
//daily_revenue.filter(daily_revenue("order_date") ==="2013-07-25 00:00:00:0").show
//daily_revenue.filter(daily_revenue("order_date") ==="2013-07-25 00:00:00:0").count

	-- limit will capture the number of records that are required
//daily_revenue.filter(daily_revenue("order_date") ==="2013-07-25 00:00:00:0").limit
